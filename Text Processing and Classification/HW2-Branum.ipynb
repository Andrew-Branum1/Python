{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ddae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Drew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Drew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Drew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Drew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews # load data from nltk data set: movie reviews\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import heapq\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from random import shuffle\n",
    "from nltk.classify import accuracy\n",
    "from nltk.classify import SklearnClassifier\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3bf772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review example: films adapted from comic books have had plenty of success , whether they ' re about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there ' s never really been a comic book like from hell before . for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid ' 80s with a 12 - part series called the watchmen . to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . in other words , don ' t dismiss this film because of its source . if you can get past the whole comic book thing , you might find another stumbling block in from hell ' s directors , albert and allen hughes . getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that ' s set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? the ghetto in question is , of course , whitechapel in 1888 london ' s east end . it ' s a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . when the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . abberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . upon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn ' t so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can ' t stomach . i don ' t think anyone needs to be briefed on jack the ripper , so i won ' t go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . in the comic , they don ' t bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . it ' s funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . and from hell ' s ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car / who made steve guttenberg a star ? \" ) . don ' t worry - it ' ll all make sense when you see it . now onto from hell ' s appearance : it ' s certainly dark and bleak enough , and it ' s surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . the print i saw wasn ' t completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don ' t say a word ) ably captures the dreariness of victorian - era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black - and - white comic . oscar winner martin childs ' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . even the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . ians holm ( joe gould ' s secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn ' t half bad . the film , however , is all good . 2 : 00 - r for strong violence / gore , sexuality , language and drug content\n",
      "Negative review example: capsule : in 2176 on the planet mars police taking into custody an accused murderer face the title menace . there is a lot of fighting and not a whole lot of story otherwise . john carpenter reprises so many ideas from his previous films , especially assault on precinct 13 , that the new film comes off as his homage to himself . , 0 ( - 4 to + 4 ) . john carpenter apparently believes that action scenes in which people fight something horrible are the same as horror scenes . for a writer and director of horror films , supposedly an expert on horror , it is a very bad mistake to make . ghosts of mars is called a horror movie , but it is more just a drawn out fight between humans and a surprisingly low - powered alien menace . in addition if anybody but john carpenter had made ghosts of mars , carpenter would have grounds to sue . this film is just chock full of pieces taken from assault on precinct 13 , the thing , and prince of darkness . it is , in fact , surprising that carpenter managed to fit so many pieces of his previous work into this film in such an admittedly novel way . but that still does not make for a really good science fiction experience . ghosts of mars takes place in the year 2176 . mars has been mostly terraformed so that humans can walk on the surface without breathing gear ( which is good for the film ' s budget ) . it is never mentioned , but the gravity on mars has been increased somehow to earth - normal , again making it easier to film . society has changed a bit by that time , but it has advanced surprisingly little . apparently the culture has changed so that women are much more in positions of control . and from carpenter ' s view , women have really made a mess of things . society has stagnated under female control so that beyond some minor technological advances society has changed less in 175 years than we might expect it to change in ten . the basic plot of ghosts of mars has much in common with that of assault on precinct 13 except that precinct 9 ( yes , precinct 9 ) has been replaced by a somewhat tacky looking rundown martian mining colony . instead of having the criminal \" napolean \" wilson , this film has the criminal \" desolation \" williams . instead of facing hoodlums with automatic weapons the police face , well , ghosts of mars . because the ghosts are somewhat alien in nature they should behave in some alien manner , but they essentially behave as human savages , in another lapse of imagination . the story is told in flashback , flashback within flashback , and flashback within flashback within flashback . ghosts of mars takes place entirely at night and is filmed almost entirely in tones of red , yellow , and black . carpenter manages to give us a powerful opening scene , showing a mining train rushing through the martian night to the sound of music with a heavy beat . sadly what follows is not really up to the buildup . the terror he creates looks a little too much like fugitive wannabes from the rock band kiss . his idea of building suspense is having a bunch of sudden jump scenes that sucker the viewer into thinking something scary is happening and then prove to be just something boring . these are standard haunted house film shock effects that require no great talent to give the audience . somewhat newer but also unimpressive are the cgi digital decapitations in some of the fights . within a short stretch of time we have seen the release of mission to mars , red planet , and ghosts of mars . after mission to mars was panned by too many reviewers it looks better and better and better as time goes by . i rate ghosts of mars a 4 on the 0 to 10 scale and a 0 on the - 4 to + 4 scale . following the movie i showed my wife , who liked ghosts of mars moderately more than i did , carpenter ' s classic assault on precinct 13 . her comment is that it was seeing the same film twice .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words):\n",
    "  return dict([(word, True) for word in words])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "print('Positive review example: ' + ' '.join(movie_reviews.words(posids[0])))\n",
    "print('Negative review example: ' + ' '.join(movie_reviews.words(negids[5])))\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db96e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.712\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##stop words\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words):\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    return dict([(word, True) for word in filtered_words])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e72b13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.724\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "                  symbol = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##lemm\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize words and convert sentences into feature vectors\n",
    "def word_feats(words):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "    return dict([(word, True) for word in lemmatized_words])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors with lemmatization\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16847d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.7\n",
      "Most Informative Features\n",
      "                outstand = True              pos : neg    =     13.6 : 1.0\n",
      "                  ludicr = True              neg : pos    =     13.0 : 1.0\n",
      "                uninvolv = True              neg : pos    =     12.3 : 1.0\n",
      "                    plod = True              neg : pos    =     11.0 : 1.0\n",
      "                  themat = True              pos : neg    =     11.0 : 1.0\n",
      "                magnific = True              pos : neg    =      9.0 : 1.0\n",
      "                  annual = True              pos : neg    =      9.0 : 1.0\n",
      "                   inept = True              neg : pos    =      9.0 : 1.0\n",
      "                   mulan = True              pos : neg    =      9.0 : 1.0\n",
      "                    numb = True              neg : pos    =      9.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##stemm\n",
    "# Initialize the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define a set of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess words (remove stop words and stem)\n",
    "def preprocess_word(word):\n",
    "    if word.lower() not in stop_words:\n",
    "        return stemmer.stem(word.lower())\n",
    "    else:\n",
    "        return None  # Return None for stop words\n",
    "\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words):\n",
    "    return dict([(preprocess_word(word), True) for word in words if preprocess_word(word) is not None])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1626023e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy after removing least informative features: 0.726\n"
     ]
    }
   ],
   "source": [
    "##remove least informative\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "\n",
    "# Get the most informative features\n",
    "most_informative_features = classifier.most_informative_features()\n",
    "\n",
    "# Remove the 10 least informative features\n",
    "least_informative_features = heapq.nsmallest(10, most_informative_features, key=lambda x: x[1])\n",
    "\n",
    "# Update feature sets by excluding the least informative features\n",
    "def update_feature_sets(feature_set, least_informative_features):\n",
    "    updated_feature_set = []\n",
    "    for feat, label in feature_set:\n",
    "        updated_feat = {k:v for k,v in feat.items() if k not in least_informative_features}\n",
    "        updated_feature_set.append((updated_feat, label))\n",
    "    return updated_feature_set\n",
    "\n",
    "trainfeats_updated = update_feature_sets(trainfeats, [feat[0] for feat in least_informative_features])\n",
    "testfeats_updated = update_feature_sets(testfeats, [feat[0] for feat in least_informative_features])\n",
    "\n",
    "# Retrain the classifier with the updated feature sets\n",
    "classifier_updated = NaiveBayesClassifier.train(trainfeats_updated)\n",
    "\n",
    "# Evaluate the classifier's accuracy\n",
    "print('accuracy after removing least informative features:', nltk.classify.util.accuracy(classifier_updated, testfeats_updated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a801636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.66\n",
      "Most Informative Features\n",
      "                 fiction = True              pos : neg    =     10.3 : 1.0\n",
      "                    girl = True              neg : pos    =      8.3 : 1.0\n",
      "                 science = True              pos : neg    =      8.3 : 1.0\n",
      "                     bad = True              neg : pos    =      8.1 : 1.0\n",
      "                     war = True              pos : neg    =      7.8 : 1.0\n",
      "                     why = True              neg : pos    =      7.8 : 1.0\n",
      "                 another = True              pos : neg    =      7.7 : 1.0\n",
      "                 cameron = True              pos : neg    =      7.7 : 1.0\n",
      "                     own = True              pos : neg    =      7.7 : 1.0\n",
      "                       + = True              pos : neg    =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##remove <3 words\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words, n):\n",
    "    # Calculate word frequencies\n",
    "    word_freq = FreqDist(words)\n",
    "\n",
    "    # Select the N most frequent terms\n",
    "    most_common_terms = heapq.nlargest(n, word_freq, key=word_freq.get)\n",
    "\n",
    "    # Filter out rare/low-frequency words (frequency of term = 1, 2, 3, etc.)\n",
    "    frequent_terms = [word for word in most_common_terms if word_freq[word] > 3]  # Adjust the threshold as needed\n",
    "\n",
    "    return dict([(word, True) for word in words if word in frequent_terms])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "n = 1000  # Adjust the value of N as needed\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f]), n), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f]), n), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54111edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.742\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     27.7 : 1.0\n",
      "                  finest = True              pos : neg    =     16.3 : 1.0\n",
      "                lifeless = True              neg : pos    =     13.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =     13.0 : 1.0\n",
      "                  turkey = True              neg : pos    =     12.3 : 1.0\n",
      "             magnificent = True              pos : neg    =     11.7 : 1.0\n",
      "              presumably = True              neg : pos    =     11.7 : 1.0\n",
      "            respectively = True              pos : neg    =     11.7 : 1.0\n",
      "                  avoids = True              pos : neg    =     10.3 : 1.0\n",
      "            breathtaking = True              pos : neg    =     10.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##50 percentile\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words, top_n, middle_percentile):\n",
    "    freq_dist = FreqDist(words)\n",
    "    \n",
    "    # Select the N most frequent terms\n",
    "    most_common_terms = [word for word, _ in freq_dist.most_common(top_n)]\n",
    "    \n",
    "    # Calculate the threshold for the middle percentile\n",
    "    middle_start = int((len(most_common_terms) - len(most_common_terms) * middle_percentile / 2) / 2)\n",
    "    middle_end = int((len(most_common_terms) + len(most_common_terms) * middle_percentile / 2) / 2)\n",
    "    \n",
    "    # Remove the middle 50% of the frequent terms\n",
    "    selected_terms = most_common_terms[:middle_start] + most_common_terms[middle_end:]\n",
    "    \n",
    "    return dict([(word, True) for word in words if word in selected_terms])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f]), top_n=2000, middle_percentile=0.5), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f]), top_n=2000, middle_percentile=0.5), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3 / 4)\n",
    "poscutoff = int(len(posfeats) * 3 / 4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8689a882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.516\n",
      "Most Informative Features\n",
      "                     bad = 0.0               pos : neg    =      1.5 : 1.0\n",
      "                    film = 0.0               neg : pos    =      1.4 : 1.0\n",
      "                    plot = 0.0               pos : neg    =      1.4 : 1.0\n",
      "                   movie = 0.0               pos : neg    =      1.4 : 1.0\n",
      "                    life = 0.0               neg : pos    =      1.3 : 1.0\n",
      "                    best = 0.0               neg : pos    =      1.3 : 1.0\n",
      "                   great = 0.0               neg : pos    =      1.2 : 1.0\n",
      "                   world = 0.0               neg : pos    =      1.2 : 1.0\n",
      "                  script = 0.0               pos : neg    =      1.2 : 1.0\n",
      "             performance = 0.0               neg : pos    =      1.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "##TF-IDF\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Extract document texts and labels\n",
    "negdocs = [' '.join(movie_reviews.words(fileids=[f])) for f in negids]\n",
    "posdocs = [' '.join(movie_reviews.words(fileids=[f])) for f in posids]\n",
    "docs = negdocs + posdocs\n",
    "labels = ['neg'] * len(negdocs) + ['pos'] * len(posdocs)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(docs, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit TF-IDF vectorizer and transform documents\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Get feature names from TF-IDF vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert TF-IDF matrix to feature dictionaries\n",
    "trainfeats = [(dict(zip(feature_names, row.toarray()[0])), label) for row, label in zip(X_train_tfidf, y_train)]\n",
    "testfeats = [(dict(zip(feature_names, row.toarray()[0])), label) for row, label in zip(X_test_tfidf, y_test)]\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fd7e413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Unigrams Only): 0.728\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##UNIGRAMS\n",
    "# Function to convert sentences into unigram feature vectors\n",
    "def unigram_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors (unigrams only)\n",
    "negfeats = [(unigram_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(unigram_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('Accuracy (Unigrams Only):', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a91ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Unigrams and Bigrams): 0.74\n",
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "              matt damon = True              pos : neg    =     13.7 : 1.0\n",
      "               not funny = True              neg : pos    =     13.7 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "           a wonderfully = True              pos : neg    =     13.0 : 1.0\n",
      "              and boring = True              neg : pos    =     13.0 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "             is terrific = True              pos : neg    =     13.0 : 1.0\n",
      "             perfect for = True              pos : neg    =     12.3 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##uni and bi grams\n",
    "# Function to convert sentences into unigram and bigram feature vectors\n",
    "def unigram_bigram_feats(words):\n",
    "    unigrams = unigram_feats(words)\n",
    "    bigrams = [' '.join(bg) for bg in list(ngrams(words, 2))]  # Extract bigrams\n",
    "    return {**unigrams, **dict([(bg, True) for bg in bigrams])}\n",
    "\n",
    "# Convert sentences into feature vectors (unigrams and bigrams)\n",
    "negfeats = [(unigram_bigram_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(unigram_bigram_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('Accuracy (Unigrams and Bigrams):', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99acc464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Unigrams, Bigrams, and Trigrams): 0.782\n",
      "Most Informative Features\n",
      "            of the worst = True              neg : pos    =     15.4 : 1.0\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "              matt damon = True              pos : neg    =     13.7 : 1.0\n",
      "               not funny = True              neg : pos    =     13.7 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "           a wonderfully = True              pos : neg    =     13.0 : 1.0\n",
      "              and boring = True              neg : pos    =     13.0 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "             is terrific = True              pos : neg    =     13.0 : 1.0\n",
      "             perfect for = True              pos : neg    =     12.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##uni, bi, trigrams\n",
    "# Function to convert sentences into unigram, bigram, and trigram feature vectors\n",
    "def unigram_bigram_trigram_feats(words):\n",
    "    unigrams = unigram_feats(words)\n",
    "    bigrams = [' '.join(bg) for bg in list(ngrams(words, 2))]  # Extract bigrams\n",
    "    trigrams = [' '.join(tg) for tg in list(ngrams(words, 3))]  # Extract trigrams\n",
    "    return {**unigrams, **dict([(bg, True) for bg in bigrams]), **dict([(tg, True) for tg in trigrams])}\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors (unigrams, bigrams, and trigrams)\n",
    "negfeats = [(unigram_bigram_trigram_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(unigram_bigram_trigram_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('Accuracy (Unigrams, Bigrams, and Trigrams):', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939c7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Trigrams Only): 0.81\n",
      "Most Informative Features\n",
      "            of the worst = True              neg : pos    =     15.4 : 1.0\n",
      "             to be funny = True              neg : pos    =     11.7 : 1.0\n",
      "              well , and = True              pos : neg    =     11.7 : 1.0\n",
      "             work with . = True              neg : pos    =     11.0 : 1.0\n",
      "               . too bad = True              neg : pos    =     10.3 : 1.0\n",
      "              . while it = True              pos : neg    =      9.7 : 1.0\n",
      "           not enough to = True              neg : pos    =      9.7 : 1.0\n",
      "     , but unfortunately = True              neg : pos    =      9.0 : 1.0\n",
      "          but the script = True              neg : pos    =      9.0 : 1.0\n",
      "             on the edge = True              pos : neg    =      9.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##TRIGRAMS\n",
    "# Function to convert sentences into trigram feature vectors\n",
    "def trigram_feats(words):\n",
    "    trigrams = [' '.join(tg) for tg in list(ngrams(words, 3))]  # Extract trigrams\n",
    "    return dict([(tg, True) for tg in trigrams])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors (trigrams only)\n",
    "negfeats = [(trigram_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(trigram_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('Accuracy (Trigrams Only):', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4a303a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Bigrams Only): 0.754\n",
      "Most Informative Features\n",
      "              matt damon = True              pos : neg    =     13.7 : 1.0\n",
      "               not funny = True              neg : pos    =     13.7 : 1.0\n",
      "           a wonderfully = True              pos : neg    =     13.0 : 1.0\n",
      "              and boring = True              neg : pos    =     13.0 : 1.0\n",
      "             is terrific = True              pos : neg    =     13.0 : 1.0\n",
      "             perfect for = True              pos : neg    =     12.3 : 1.0\n",
      "                a boring = True              neg : pos    =     11.7 : 1.0\n",
      "           absolutely no = True              neg : pos    =     11.0 : 1.0\n",
      "              boring and = True              neg : pos    =     11.0 : 1.0\n",
      "             brilliant , = True              pos : neg    =     11.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##BIGRAMS\n",
    "# Function to convert sentences into bigram feature vectors\n",
    "def bigram_feats(words):\n",
    "    bigrams = [' '.join(bg) for bg in list(ngrams(words, 2))]  # Extract bigrams\n",
    "    return dict([(bg, True) for bg in bigrams])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors (bigrams only)\n",
    "negfeats = [(bigram_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(bigram_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('Accuracy (Bigrams Only):', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee4faec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Trigrams Only - Without Punctuation): 0.816\n",
      "Most Informative Features\n",
      "            of the worst = True              neg : pos    =     15.4 : 1.0\n",
      "             to be funny = True              neg : pos    =     12.3 : 1.0\n",
      "            that s about = True              neg : pos    =     10.3 : 1.0\n",
      "           not enough to = True              neg : pos    =      9.7 : 1.0\n",
      "          but the script = True              neg : pos    =      9.0 : 1.0\n",
      "             on the edge = True              pos : neg    =      9.0 : 1.0\n",
      "                as he is = True              pos : neg    =      8.6 : 1.0\n",
      "            to work with = True              neg : pos    =      8.4 : 1.0\n",
      "         and even though = True              pos : neg    =      8.3 : 1.0\n",
      "              but with a = True              pos : neg    =      8.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "##TRIGRAM w/o STOP WORDS\n",
    "# Function to remove punctuation from words\n",
    "def remove_punctuation(words):\n",
    "    return [word for word in words if word not in string.punctuation]\n",
    "\n",
    "# Function to convert sentences into trigram feature vectors\n",
    "def trigram_feats(words):\n",
    "    # Remove punctuation from words\n",
    "    words = remove_punctuation(words)\n",
    "    trigrams = [' '.join(tg) for tg in list(ngrams(words, 3))]  # Extract trigrams\n",
    "    return dict([(tg, True) for tg in trigrams])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors (trigrams only)\n",
    "negfeats = [(trigram_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(trigram_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "# Train and evaluate the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print('Accuracy (Trigrams Only - Without Punctuation):', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dda9124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.864\n"
     ]
    }
   ],
   "source": [
    "##SVM\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Support Vector Machine classifier\n",
    "svm_classifier = SklearnClassifier(SVC())\n",
    "svm_classifier.train(trainfeats)\n",
    "print('accuracy:', accuracy(svm_classifier, testfeats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eca8378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.508\n"
     ]
    }
   ],
   "source": [
    "#SVM Trigrams\n",
    "# Function to convert sentences into feature vectors by considering trigrams as features.\n",
    "def trigram_feats(words):\n",
    "    trigrams = [' '.join(tg) for tg in list(ngrams(words, 3))]  # Extract trigrams\n",
    "    return dict([(tg, True) for tg in trigrams])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors (trigrams only)\n",
    "negfeats = [(trigram_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(trigram_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Support Vector Machine classifier\n",
    "svm_classifier = SklearnClassifier(SVC())\n",
    "svm_classifier.train(trainfeats)\n",
    "print('accuracy:', accuracy(svm_classifier, testfeats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80a99da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.866\n"
     ]
    }
   ],
   "source": [
    "##SVM stop words\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words):\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    return dict([(word, True) for word in filtered_words])\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Support Vector Machine classifier\n",
    "svm_classifier = SklearnClassifier(SVC())\n",
    "svm_classifier.train(trainfeats)\n",
    "print('accuracy:', accuracy(svm_classifier, testfeats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88504aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.896\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Logistic Regression classifier\n",
    "logistic_classifier = SklearnClassifier(LogisticRegression())\n",
    "logistic_classifier.train(trainfeats)\n",
    "print('accuracy:', accuracy(logistic_classifier, testfeats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44ec8714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words):\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    return dict([(word, True) for word in filtered_words])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Logistic Regression classifier\n",
    "logistic_classifier = SklearnClassifier(LogisticRegression())\n",
    "logistic_classifier.train(trainfeats)\n",
    "print('accuracy:', accuracy(logistic_classifier, testfeats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07f3842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression + bigram\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def trigram_feats(words):\n",
    "    trigrams = [' '.join(tg) for tg in list(ngrams(words, 2))]  # Extract trigrams\n",
    "    return dict([(tg, True) for tg in trigrams])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(trigram_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(trigram_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Logistic Regression classifier\n",
    "logistic_classifier = SklearnClassifier(LogisticRegression())\n",
    "logistic_classifier.train(trainfeats)\n",
    "print('accuracy:', accuracy(logistic_classifier, testfeats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ade41b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.802\n"
     ]
    }
   ],
   "source": [
    "##BernoulliNB\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def word_feats(words):\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    return dict([(word, True) for word in filtered_words])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Bernoulli Naive Bayes classifier\n",
    "bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_classifier.train(trainfeats)\n",
    "print('accuracy:', accuracy(bernoulli_classifier, testfeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "363bfcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.742\n"
     ]
    }
   ],
   "source": [
    "#BernoulliNB + bigrams\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to convert sentences into feature vectors by considering every word as a feature.\n",
    "def bigram_feats(words):\n",
    "    bigrams = [' '.join(tg) for tg in list(ngrams(words, 2))]  # Extract biigrams\n",
    "    return dict([(tg, True) for tg in bigrams])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(bigram_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(bigram_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Bernoulli Naive Bayes classifier\n",
    "bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_classifier.train(trainfeats)\n",
    "print('accuracy:', accuracy(bernoulli_classifier, testfeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89a8b46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n",
      "accuracy: 0.536\n"
     ]
    }
   ],
   "source": [
    "#BernoulliNB + trigrams\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def trigram_feats(words):\n",
    "    trigrams = [' '.join(tg) for tg in list(ngrams(words, 3))]  # Extract trigrams\n",
    "    return dict([(tg, True) for tg in trigrams])\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Convert sentences into feature vectors\n",
    "negfeats = [(trigram_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(trigram_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# Create the dataset and split it into training and testing\n",
    "negcutoff = int(len(negfeats) * 3/4)\n",
    "poscutoff = int(len(posfeats) * 3/4)\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train and evaluate the Bernoulli Naive Bayes classifier\n",
    "bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_classifier.train(trainfeats)\n",
    "print('accuracy:', accuracy(bernoulli_classifier, testfeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c73e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess and convert documents into a list of strings\n",
    "def preprocess_documents(fileids):\n",
    "    documents = []\n",
    "    for fileid in fileids:\n",
    "        words = movie_reviews.words(fileids=[fileid])\n",
    "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "        documents.append(' '.join(filtered_words))\n",
    "    return documents\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Preprocess documents\n",
    "neg_docs = preprocess_documents(negids)\n",
    "pos_docs = preprocess_documents(posids)\n",
    "docs = neg_docs + pos_docs\n",
    "labels = ['neg'] * len(neg_docs) + ['pos'] * len(pos_docs)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "docs_train, docs_test, labels_train, labels_test = train_test_split(docs, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert sentences into feature vectors using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(docs_train)\n",
    "X_test = vectorizer.transform(docs_test)\n",
    "\n",
    "# Train and evaluate the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, labels_train)\n",
    "predicted_labels = clf.predict(X_test)\n",
    "print('accuracy:', accuracy_score(labels_test, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79be884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to preprocess and convert documents into a list of strings\n",
    "def preprocess_documents(fileids):\n",
    "    documents = []\n",
    "    for fileid in fileids:\n",
    "        words = movie_reviews.words(fileids=[fileid])\n",
    "        filtered_words = [word.lower() for word in words if word.isalpha()]  # Stop words are not removed\n",
    "        documents.append(' '.join(filtered_words))\n",
    "    return documents\n",
    "\n",
    "# Extract positive and negative sentences\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# Preprocess documents\n",
    "neg_docs = preprocess_documents(negids)\n",
    "pos_docs = preprocess_documents(posids)\n",
    "docs = neg_docs + pos_docs\n",
    "labels = ['neg'] * len(neg_docs) + ['pos'] * len(pos_docs)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "docs_train, docs_test, labels_train, labels_test = train_test_split(docs, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert sentences into feature vectors using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(docs_train)\n",
    "X_test = vectorizer.transform(docs_test)\n",
    "\n",
    "# Train and evaluate the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, labels_train)\n",
    "predicted_labels = clf.predict(X_test)\n",
    "print('accuracy:', accuracy_score(labels_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935d78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
